CREATE A UDF IN SCALA AND USE IT IN SPARK SQL

	def convertTime=(s:Long)=>{
	  s/MS_IN_HOUR * MS_IN_HOUR
	}
	spark.udf.register("convertTimeUDF",convertTime)

	inputDF.createOrReplaceTempView("activity")
	val activityByProduct = spark.sql("""SELECT
											product,
											convertTimeUDF(timestamp_hour) timestamp_hour,
											sum(case when action = 'purchase' then 1 else 0 end) as purchase_count,
											sum(case when action = 'add_to_cart' then 1 else 0 end) as add_to_cart_count,
											sum(case when action = 'page_view' then 1 else 0 end) as page_view_count
											from activity
											group by product, timestamp_hour """)

CONVERT CASE CLASS INTO SCHEMA

	import org.apache.spark.sql.types.StructType
	import org.apache.spark.sql.catalyst.ScalaReflection

	case class A(key: String, time: java.sql.Timestamp, date: java.sql.Date, decimal: java.math.BigDecimal, map: Map[String, Int], nested: Seq[Map[String, Seq[Int]]])
	val schema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]
	schema.printTreeString
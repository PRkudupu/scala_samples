CREATE A UDF IN SCALA AND USE IT IN SPARK SQL

	def convertTime=(s:Long)=>{
	  s/MS_IN_HOUR * MS_IN_HOUR
	}
	spark.udf.register("convertTimeUDF",convertTime)

	inputDF.createOrReplaceTempView("activity")
	val activityByProduct = spark.sql("""SELECT
											product,
											convertTimeUDF(timestamp_hour) timestamp_hour,
											sum(case when action = 'purchase' then 1 else 0 end) as purchase_count,
											sum(case when action = 'add_to_cart' then 1 else 0 end) as add_to_cart_count,
											sum(case when action = 'page_view' then 1 else 0 end) as page_view_count
											from activity
											group by product, timestamp_hour """)

CONVERT CASE CLASS INTO SCHEMA

	import org.apache.spark.sql.types.StructType
	import org.apache.spark.sql.catalyst.ScalaReflection

	case class A(key: String, time: java.sql.Timestamp, date: java.sql.Date, decimal: java.math.BigDecimal, map: Map[String, Int], nested: Seq[Map[String, Seq[Int]]])
	val schema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]
	schema.printTreeString
	
OPTION
	Container of zero or one element of a given type.
	An option can either be Some[T] or None object
	
	EX
		val capitals = Map("France" -> "Paris", "Japan" -> "Tokyo")
      
		println("capitals.get( \"France\" ) : " +  capitals.get( "France" ))
		println("capitals.get( \"India\" ) : " +  capitals.get( "India" ))
	
	OUTPUT
		capitals.get( "France" ) : Some(Paris)
		capitals.get( "India" ) : None
		
	Note:
		Most common way to take optional values apart is through pattern match
		object Demo {
		   def main(args: Array[String]) {
			  val capitals = Map("France" -> "Paris", "Japan" -> "Tokyo")
			  
			  println("show(capitals.get( \"Japan\")) : " + show(capitals.get( "Japan")) )
			  println("show(capitals.get( \"India\")) : " + show(capitals.get( "India")) )
		   }
		   
		   def show(x: Option[String]) = x match {
			  case Some(s) => s
			  case None => "?"
		   }
		}
		
		
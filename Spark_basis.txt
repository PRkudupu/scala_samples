case class Department(id:Int,name:String)
case class Employee(id:Int,f_name:String,l_name:String)

// Create departments 
// Create employees

val dep1=new Department(1,"IT")
val dep2=new Department(2,"HR")
val dep3=new Department(3,"Hardware")
val dep4=new Department(4,"Marketing")


val emp1 =new Employee(1,"prathap","kudupu")
val emp2 =new Employee(2,"sandeep","shetty")
val emp3 =new Employee(3,"sacheen","achan")
val emp4 =new Employee(4,"jatheen","achan")


// Create dataframes for departments
val dep=Seq(dep1,dep2)
val dp_df1 =dep.toDF()
display(df1)

val emp=Seq(emp1,emp2)
val dp_df1=emp.toDF()

display(dp_df1)


// UNION OF TO DF
val emp_un=Seq(emp3,emp4)
val dp_df2=emp_un.toDF()

val un_DF=dp_df1.unionAll(dp_df2)
display(un_DF)


//Connect to azure blob and try to load the data
val containername ="sample"
val storageaccountname = "kudupu"
val storageaccountaccesskey = "<keys>"

val filelocation = "https://kudupu.blob.core.windows.net/sample/C2ImportCalEventSample.csv"
val file_type = "csv"


spark.conf.set(
  "fs.azure.account.key."+storage_account_name+".blob.core.windows.net",
  storage_account_access_key)


val sdf = spark.read.csv("wasbs://containername@storageaccountname.blob.core.windows.net/filelocation")


// LOOK AT THE FILE SYSTEM
display(dbutils.fs.ls("/databricks-datasets/samples/docs/"))

// TRANSFORMATION
val df =spark.read.text("/databricks-datasets/samples/docs/")

//ACTION
df.count()
df.take(5)

//LIMIT THE CONTENTS OF THE RDD
val lm =df.limit(2)
lm.show()

//FILTER

//WITH COLUMN
.withColumn("rolling_range",lit(range24))

//CHECK FOR ISNULL
sourceDF.withColumn("is_person_name_null",col("person_name").isNull).show()

//REPLACE NULL VALUES
.withColumn("primary_channel", when(col("primary_channel").isNull, "NONE").otherwise(col("primary_channel")))

// CREATE SPARKSESSION

import org.apache.spark.sql.{DataFrame, SaveMode, SparkSession}
val sparkSession=SparkSession
val spark = SparkSession
  .builder()
  .appName("Spark SQL basic example")
  .config("spark.some.config.option", "some-value")
  .getOrCreate()

omniTransactions.createOrReplaceTempView("omni")
val result= spark.sql(""" select *
                    from omni""") 

result.show()

//CONDITIONAL QUERY GENERATION
val customerLoyaltyFiletered=sparkSession.sql("""select * from customer_loyalty_details_t """ + historicalFilter)


//CONVER TO UNIX TIMESTAMP
 .withColumn("rpting_dt",from_unixtime(unix_timestamp(),"yyyy-MM-dd HH:mm:ss").cast("timestamp"))
